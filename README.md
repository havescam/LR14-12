1) Задание 13: Adversarial Examples (Адверсарные примеры)
Задача: создать систему генерации adversarial примеров с использованием FGSM и PGD атак.
Требования:
Использовать градиентные методы для генерации adversarial примеров
Реализовать FGSM (Fast Gradient Sign Method) и PGD (Projected Gradient Descent)
Оценка устойчивости модели
Что нужно дополнить:
Реализацию FGSM и PGD атак
Создание adversarial примеров
Оценку устойчивости модели
Визуализацию результатов атак​

2.Алгоритм и архитектура работы НС по блокам
Пошаговое описание алгоритма работы нейросети(НС)
1.Подготовка данных.
Собирается датасет MNIST (28×28 изображения цифр, нормализованные в ). Создаётся CNN: Conv2D(32)→MaxPool→Conv2D(64)→MaxPool→Flatten→Dense(64)→Dense(10,softmax). Модель обучается на оригинальных данных с sparse_categorical_crossentropy, validation_split=0.1.​

2.Прямой проход (предсказание)
Изображение подаётся в CNN: сверточные слои извлекают пространственные признаки, MaxPool уменьшает размерность, Dense-слои формируют вероятности классов через softmax. Для атаки: через GradientTape фиксируется прямой проход, вычисляется loss=sparse_categorical_crossentropy(target, prediction).​

3.Расчёт ошибки
Для FGSM: loss между истинным лейблом и предсказанием на оригинальном изображении. Градиент ∂loss/∂image используется как направление атаки. Для оценки: SparseCategoricalAccuracy считается отдельно для оригинальных изображений и adversarial примеров (FGSM/PGD), показывая падение точности.​

4.Обратное распространение и обновление весов
Градиенты вычисляются по входному изображению (не по весам модели): sign(∇loss) масштабируется коэффициентом ε для FGSM (один шаг). PGD: итеративно (10 шагов) применяет малый шаг α×sign(∇), проецирует на l∞-шар ε вокруг оригинала. Модель остаётся неизменной.​

2)Обучение по эпохам
Модель обучается 5 эпох на оригинальных данных. После обучения создаётся attacker, тестируется устойчивость на 100 тестовых изображениях. Считаются accuracy: original, FGSM(ε=0.1), PGD(ε=0.1). Строится барчарт сравнения точностей, демонстрирующий уязвимость.​

Архитектура
Класс AdversarialAttack реализует архитектуру системы генерации и оценки адверсарных примеров для CNN-модели классификации изображений MNIST.

Основные компоненты и методы:

init(model):
Инициализирует ссылку на обученную CNN-модель для прямого прохода и вычисления градиентов через GradientTape.​

fgsm_attack(image, label, epsilon=0.01):
Fast Gradient Sign Method: через GradientTape вычисляет градиент sparse_categorical_crossentropy по пикселям изображения, формирует adversarial пример: image + ε×sign(∇loss), клипит в. Одношаговая атака максимальной силы в пределах ε.​

pgd_attack(image, label, epsilon=0.01, alpha=0.01, num_iter=10):
Projected Gradient Descent: итеративно (10 шагов) применяет FGSM с малым шагом α, проецирует perturbation на l∞-шар радиуса ε: clip(image + δ, image-ε, image+ε), финальный клип в. Более точная и сильная атака.​

evaluate_robustness(test_images, test_labels, epsilon=0.01):
Батчевая оценка устойчивости: для каждого изображения считает SparseCategoricalAccuracy на оригинале, FGSM и PGD примерах. Возвращает словарь {original_accuracy, fgsm_accuracy, pgd_accuracy} для анализа падения точности.​

plot_loss_and_accuracy():
Визуализирует метрики устойчивости: барчарт сравнения точностей original/FGSM/PGD. Показывает динамику деградации производительности при adversarial атаках.​

Вспомогательные элементы архитектуры
GradientTape: дифференцируемый трекинг для градиентов по входу.
SparseCategoricalAccuracy: метрика для integer лейблов без one-hot.
tf.clip_by_value: ограничение perturbation реалистичными значениями.​
CNN-модель: стандартный классификатор MNIST как мишень атак (Conv2D→Dense).

3)Ответ на контрольный вопрос
Что такое алгоритм Кнута-Морриса-Пратта (KMP) и для чего он используется?
Алгоритм Кнута-Морриса-Пратта (KMP) — линейный алгоритм поиска подстроки в тексте, использующий префиксную функцию для эффективного пропуска ненужных сравнений.​

Формулировка
Для текста T длины n и паттерна P длины m строится префиксная функция π[i] — длина наибольшего собственного префикса P[0..i], совпадающего с суффиксом. Поиск: при несовпадении на позиции k сдвиг = k - π[k-1], а не полный откат. Сложность: O(n+m).​

Для чего используется
Быстрый поиск подстрок в больших текстах, геномных последовательностях, логах, поисковых системах, компиляторах (лексер). Преимущество: использует внутреннюю структуру паттерна для избежания O(n×m) сравнений наивного алгоритма.
